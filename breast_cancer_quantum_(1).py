# -*- coding: utf-8 -*-
"""breast_cancer_quantum (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d6XjO_y-foCILIZvscq2I_CDgmPOmmSw

# quantum part

# package installation
"""

!pip install pennylane

pip install tensorflow

"""# QNN"""

import pennylane as qml
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Load the breast cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the quantum device
dev = qml.device("default.qubit", wires=4)

# Define the quantum circuit
@qml.qnode(dev)
@qml.qnode(dev)
def quantum_circuit(inputs, weights):
    for i in range(len(inputs)):
        qml.RX(inputs[i], wires=i)
    for i in range(len(inputs)):
        # Explicitly provide the wire argument
        qml.Rot(weights[i][0], weights[i][1], weights[i][2], wires=i)
    return [qml.expval(qml.PauliZ(i)) for i in range(len(inputs))]

# Define the QCNN model
def q_cnn_model():
    model = Sequential([
        Conv1D(filters=4, kernel_size=2, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
        MaxPooling1D(pool_size=4),
        Flatten(),
        Dense(2, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    return model

# Reshape the data for the QCNN model
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Instantiate the QCNN model
model = q_cnn_model()

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_reshaped, y_test)
print("QCNN Test Accuracy: {:.2f}%".format(accuracy * 100))

# You will need to define inputs and weights before drawing the circuit
inputs = np.random.random(4)
# Adjust the shape of weights to match the requirement of BasicEntanglerLayers
weights = np.random.random((4, 4))  # 4 layers, each with parameters for 4 wires
print(qml.draw(quantum_circuit)(inputs, weights))

"""plotting"""

y_pred = model.predict(X_test_reshaped)
from sklearn.metrics import roc_curve, auc

# Compute ROC curve and AUC for the classical model predictions
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""metrics

"""

from sklearn.metrics import precision_recall_fscore_support

# Convert probabilities to binary predictions (0 or 1) using a threshold
y_pred_binary = (y_pred >= 0.5).astype(int)

# Compute precision, recall, and F1 score for the binary predictions
precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred_binary, average='binary')

print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1 Score: {:.2f}".format(fscore))

"""# QSVM"""

import pennylane as qml
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Load the breast cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dimensionality reduction using PCA
n_qubits = 4  # Number of qubits to use
pca = PCA(n_components=n_qubits)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Define the quantum device
dev = qml.device("default.qubit", wires=n_qubits)

# Define the quantum circuit for feature encoding
@qml.qnode(dev)
def quantum_circuit(inputs, weights):
    for i in range(len(inputs)):
        qml.RX(inputs[i], wires=i)
    for i in range(len(inputs)):
        qml.Rot(weights[i][0], weights[i][1], weights[i][2], wires=i)
    return [qml.expval(qml.PauliZ(i)) for i in range(len(inputs))]

# Define a function to encode classical data into quantum features
def encode_data_to_quantum(data, weights):
    quantum_features = []
    for sample in data:
        circuit_output = quantum_circuit(sample, weights)
        quantum_features.append(circuit_output)
    return np.array(quantum_features)

# Train a QSVM model
def train_qsvm(X_train, y_train, X_test, y_test, initial_weights):
    # Encode training and testing data into quantum features
    X_train_quantum = encode_data_to_quantum(X_train, initial_weights)
    X_test_quantum = encode_data_to_quantum(X_test, initial_weights)

    # Train a classical SVM model using the quantum features
    svm_model = SVC(kernel='linear')
    svm_model.fit(X_train_quantum, y_train)

    # Calculate approximate hinge loss on training data
    y_pred_train = svm_model.decision_function(X_train_quantum)
    hinge_loss = np.mean(np.maximum(0, 1 - y_train * y_pred_train))

    # Evaluate the model
    accuracy = svm_model.score(X_test_quantum, y_test)

    return svm_model, accuracy, hinge_loss

# Generate random initial weights for the quantum circuit
np.random.seed(0)
initial_weights = np.random.randn(n_qubits, 3)  # Adjusted for the number of qubits

# Train the QSVM model using PCA-reduced data
svm_model, accuracy, loss = train_qsvm(X_train_pca, y_train, X_test_pca, y_test, initial_weights)

# Print the accuracy and loss of the trained QSVM model
print("Accuracy of the QSVM model: {:.2f}%".format(accuracy * 100))
print("Approximate Hinge Loss:", loss)

# Visualize pair plot for PCA-reduced data
pca_df = pd.DataFrame(X_train_pca, columns=[f"PCA{i+1}" for i in range(n_qubits)])
pca_df['target'] = y_train

sns.pairplot(pca_df, hue='target', palette='viridis')
plt.suptitle('Pair Plot of PCA-reduced Breast Cancer Dataset', y=1.02)
plt.show()

print(qml.draw(quantum_circuit)(X_train_pca[:4], initial_weights))

"""metrices"""

# Generate X_test_quantum using the encode_data_to_quantum function
X_test_quantum = encode_data_to_quantum(X_test_pca, initial_weights)

# Predict
y_pred_qsvm = svm_model.predict(X_test_quantum)

# Calculate metrics
accuracy_qsvm = accuracy_score(y_test, y_pred_qsvm)
precision_qsvm = precision_score(y_test, y_pred_qsvm)
recall_qsvm = recall_score(y_test, y_pred_qsvm)
f1_qsvm = f1_score(y_test, y_pred_qsvm)

# Print the metrics
print("QSVM Test Accuracy: {:.2f}%".format(accuracy_qsvm * 100))
print("QSVM Precision: {:.2f}%".format(precision_qsvm*100))
print("QSVM Recall: {:.2f}%".format(recall_qsvm*100))
print("QSVM F1-score: {:.2f}%".format(f1_qsvm*100))

"""plots"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred_qsvm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import roc_curve, auc

# Get the predicted probabilities for the positive class
y_pred_prob = svm_model.decision_function(X_test_quantum)

# Calculate the false positive rate (FPR) and true positive rate (TPR)
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

# Calculate the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import precision_recall_curve

# Calculate precision and recall
precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)

# Plot Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='upper right')
plt.show()

"""# VQC"""

import pennylane as qml
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and preprocess the data
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the number of qubits and layers
n_qubits = 4
n_layers = 2

# Define the quantum device
dev = qml.device("default.qubit", wires=n_qubits)

# Define the quantum circuit
@qml.qnode(dev)
def quantum_circuit(inputs, weights):
    # Encode the input data
    for i in range(n_qubits):
        qml.RX(inputs[i], wires=i)

    # Apply variational layers
    for layer in range(n_layers):
        for i in range(n_qubits):
            qml.Rot(*weights[layer][i], wires=i)
        for i in range(n_qubits - 1):
            qml.CNOT(wires=[i, i + 1])

    # Measure all qubits
    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

# Define the variational classifier
def variational_classifier(inputs, weights):
    return quantum_circuit(inputs, weights)

# Define the binary cross-entropy loss function
def binary_cross_entropy(predictions, targets):
    predictions = np.clip(predictions, 1e-10, 1 - 1e-10)
    return np.mean(-(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions)))

# Define the cost function
def cost(weights, X, y):
    predictions = np.array([variational_classifier(x, weights) for x in X])
    predictions = (np.mean(predictions, axis=1) + 1) / 2  # Map from [-1, 1] to [0, 1]
    return binary_cross_entropy(predictions, y)

# Initialize weights
np.random.seed(42)
weight_shapes = {"weights": (n_layers, n_qubits, 3)}
weights = np.random.uniform(0, 2 * np.pi, size=(n_layers, n_qubits, 3))

# Create the optimizer
opt = qml.GradientDescentOptimizer(stepsize=0.1)

# Train the model
epochs = 10
batch_size = 32

for epoch in range(epochs):
    batch_index = np.random.randint(0, len(X_train_scaled), (batch_size,))
    X_batch = X_train_scaled[batch_index][:, :n_qubits]
    y_batch = y_train[batch_index]

    weights = opt.step(lambda w: cost(w, X_batch, y_batch), weights)

    if (epoch + 1) % 10 == 0:
        cost_value = cost(weights, X_train_scaled[:, :n_qubits], y_train)
        print(f"Epoch {epoch + 1}/{epochs}, Cost: {cost_value:.4f}")

# Evaluate the model
def accuracy(X, y, weights):
    predictions = np.array([variational_classifier(x, weights) for x in X])
    predictions = (np.mean(predictions, axis=1) + 1) / 2  # Map from [-1, 1] to [0, 1]
    return np.mean((predictions > 0.5) == y)

train_accuracy = accuracy(X_train_scaled[:, :n_qubits], y_train, weights)
test_accuracy = accuracy(X_test_scaled[:, :n_qubits], y_test, weights)

print(f"Train accuracy: {train_accuracy:.4f}")
print(f"Test accuracy: {test_accuracy:.4f}")

# Plot the circuit
print(qml.draw(quantum_circuit)(np.zeros(n_qubits), np.zeros((n_layers, n_qubits, 3))))

import numpy as np
import matplotlib.pyplot as plt

# Draw the circuit
fig, ax = qml.draw_mpl(quantum_circuit)(inputs, weights)

# Adjust the figure size if needed
fig.set_size_inches(20, 10)

# Add a title
plt.title("Variational Quantum Classifier Circuit", fontsize=16)

# Show the plot
plt.tight_layout()
plt.show()

import pennylane as qml
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def predict(X, weights):
    predictions = np.array([variational_classifier(x, weights) for x in X])
    return (np.mean(predictions, axis=1) > 0)  # Binary prediction

# Make predictions
y_train_pred = predict(X_train_scaled[:, :n_qubits], weights)
y_test_pred = predict(X_test_scaled[:, :n_qubits], weights)

# Calculate metrics
train_accuracy = np.mean(y_train_pred == y_train)
test_accuracy = np.mean(y_test_pred == y_test)

train_precision = precision_score(y_train, y_train_pred)
test_precision = precision_score(y_test, y_test_pred)

train_recall = recall_score(y_train, y_train_pred)
test_recall = recall_score(y_test, y_test_pred)

train_f1 = f1_score(y_train, y_train_pred)
test_f1 = f1_score(y_test, y_test_pred)

# Print metrics
print(f"Train Metrics:")
print(f"  Accuracy:  {train_accuracy:.4f}")
print(f"  Precision: {train_precision:.4f}")
print(f"  Recall:    {train_recall:.4f}")
print(f"  F1-score:  {train_f1:.4f}")

print(f"\nTest Metrics:")
print(f"  Accuracy:  {test_accuracy:.4f}")
print(f"  Precision: {test_precision:.4f}")
print(f"  Recall:    {test_recall:.4f}")
print(f"  F1-score:  {test_f1:.4f}")

# Calculate and plot confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# Plot ROC curve
from sklearn.metrics import roc_curve, auc

# Get probabilities for ROC curve
y_test_proba = np.array([variational_classifier(x, weights) for x in X_test_scaled[:, :n_qubits]])
y_test_proba = (np.mean(y_test_proba, axis=1) + 1) / 2  # Map from [-1, 1] to [0, 1]

fpr, tpr, _ = roc_curve(y_test, y_test_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""# results

"""

import pandas as pd

# Define the metrics for each model
metrics = {
    "Model": ["VQC", "QCNN", "QSVM"],
    "Accuracy (%)": [accuracy * 100, accuracy_qcnn * 100, accuracy_qsvm * 100],
    "Precision (%)": [precision * 100, precision_qcnn * 100, precision_qsvm * 100],
    "Recall (%)": [recall * 100, recall_qcnn * 100, recall_qsvm * 100],
    "F1-score (%)": [f1 * 100, f1_qcnn * 100, f1_qsvm * 100]
}

# Create a DataFrame
metrics_df = pd.DataFrame(metrics)

# Set the model column as the index
metrics_df.set_index("Model", inplace=True)

# Print the DataFrame
print(metrics_df)

# Find the best model based on the highest accuracy
best_model = metrics_df.loc[metrics_df["Accuracy (%)"].idxmax()]

# Print the best model
print("\nBest Model (Based on Accuracy):")
print(best_model)

import matplotlib.pyplot as plt

# Plotting the metrics
metrics_df.plot(kind="bar", figsize=(10, 6))
plt.title("Performance Metrics of Quantum Models")
plt.ylabel("Percentage")
plt.xlabel("Model")
plt.xticks(rotation=0)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.legend(loc="upper right")
plt.tight_layout()
plt.show()

"""# classical part

# random forest  , knn
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load Breast Cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
rf_model = RandomForestClassifier(random_state=42)
dt_model = DecisionTreeClassifier(random_state=42)
knn_model = KNeighborsClassifier()

# Train and evaluate Random Forest
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_test_scaled)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print("Random Forest Test Accuracy: {:.2f}%".format(accuracy_rf * 100))
print("Random Forest Precision: {:.2f}%".format(precision_rf * 100))
print("Random Forest Recall: {:.2f}%".format(recall_rf * 100))
print("Random Forest F1-score: {:.2f}%".format(f1_rf * 100))

# Train and evaluate Decision Tree
dt_model.fit(X_train_scaled, y_train)
y_pred_dt = dt_model.predict(X_test_scaled)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)

print("\nDecision Tree Test Accuracy: {:.2f}%".format(accuracy_dt * 100))
print("Decision Tree Precision: {:.2f}%".format(precision_dt * 100))
print("Decision Tree Recall: {:.2f}%".format(recall_dt * 100))
print("Decision Tree F1-score: {:.2f}%".format(f1_dt * 100))

# Train and evaluate KNN
knn_model.fit(X_train_scaled, y_train)
y_pred_knn = knn_model.predict(X_test_scaled)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)

print("\nKNN Test Accuracy: {:.2f}%".format(accuracy_knn * 100))
print("KNN Precision: {:.2f}%".format(precision_knn * 100))
print("KNN Recall: {:.2f}%".format(recall_knn * 100))
print("KNN F1-score: {:.2f}%".format(f1_knn * 100))

from sklearn.model_selection import learning_curve

# Define a function to plot learning curve
def plot_learning_curve(model, X, y):
    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, n_jobs=-1,
                                                            train_sizes=np.linspace(0.1, 1.0, 10),
                                                            scoring='accuracy')
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(8, 6))
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="blue")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="green")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="blue", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="green", label="Cross-validation score")
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    plt.title("Learning Curve")
    plt.legend(loc="best")
    plt.show()

# Plot learning curve for Random Forest
plot_learning_curve(rf_model, X_train_scaled, y_train)

from sklearn.metrics import roc_curve, auc

# Compute ROC curve and AUC for Random Forest
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_model.predict_proba(X_test_scaled)[:, 1])
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc_rf))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) - Random Forest')
plt.legend(loc="lower right")
plt.show()

"""# svm"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load Breast Cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# PCA for dimensionality reduction
pca = PCA(n_components=5)  # Select 5 components for a more manageable pair plot
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Create a DataFrame for the PCA components
df_pca = pd.DataFrame(X_train_pca, columns=[f'PCA{i+1}' for i in range(X_train_pca.shape[1])])
df_pca['target'] = y_train

# Pair Plot for PCA components
sns.pairplot(df_pca, hue='target', palette='coolwarm', diag_kind='kde')
plt.suptitle('Pair Plot of Breast Cancer Dataset (PCA Components)', y=1.02)
plt.show()

"""plots"""

from sklearn.metrics import roc_curve, roc_auc_score

# Calculate the probabilities of the positive class
y_prob_svm = svm_classifier.decision_function(X_test_scaled)

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob_svm)

# Calculate the area under the ROC curve (AUC)
auc = roc_auc_score(y_test, y_prob_svm)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = {:.2f})'.format(auc))
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for SVM Model')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""CNN

# cnn
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load Breast Cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for CNN (samples, time steps, features)
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Convert labels to categorical (one-hot encoding)
y_train_categorical = to_categorical(y_train)
y_test_categorical = to_categorical(y_test)

# Build the CNN model
model = Sequential([
    Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(2, activation='softmax')  # 2 output units for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_cnn, y_train_categorical, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_cnn, y_test_categorical)
print("CNN Test Accuracy: {:.2f}%".format(accuracy * 100))

# Predict on test data
y_pred_cnn = model.predict(X_test_cnn)
y_pred_cnn_classes = np.argmax(y_pred_cnn, axis=1)

# Calculate metrics
accuracy_cnn = accuracy_score(y_test, y_pred_cnn_classes)
precision_cnn = precision_score(y_test, y_pred_cnn_classes)
recall_cnn = recall_score(y_test, y_pred_cnn_classes)
f1_cnn = f1_score(y_test, y_pred_cnn_classes)

print("CNN Test Accuracy: {:.2f}%".format(accuracy_cnn * 100))
print("CNN Precision: {:.2f}%".format(precision_cnn * 100))
print("CNN Recall: {:.2f}%".format(recall_cnn * 100))
print("CNN F1-score: {:.2f}%".format(f1_cnn * 100))

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""# classical result"""

import pandas as pd

# Define the metrics for each model
metrics = {
    "Model": ["SVM", "Random Forest", "KNN", "Neural Network"],
    "Accuracy (%)": [accuracy_svm * 100, rf_accuracy * 100, knn_accuracy * 100, accuracy * 100],
    "Precision (%)": [precision_svm * 100, rf_precision * 100, knn_precision * 100, precision * 100],
    "Recall (%)": [recall_svm * 100, rf_recall * 100, knn_recall * 100, recall * 100],
    "F1-score (%)": [f1_svm * 100, rf_f1_score * 100, knn_f1_score * 100, f1 * 100]
}

# Create a DataFrame
metrics_df = pd.DataFrame(metrics)

# Set the model column as the index
metrics_df.set_index("Model", inplace=True)

# Print the DataFrame
print(metrics_df)
# Sort the DataFrame by accuracy
metrics_df_sorted = metrics_df.sort_values(by="Accuracy (%)", ascending=False)

# Get the best model (the one with the highest accuracy)
best_model = metrics_df_sorted.iloc[0]

# Print the best model
print("Best Model (Based on Accuracy):")
print(best_model)

"""# comparision of classical and quantum"""

import pandas as pd

# Define the metrics for classical models
classical_metrics = {
    "Model": ["SVM", "Random Forest", "KNN", "Neural Network"],
    "Accuracy (%)": [accuracy_svm * 100, rf_accuracy * 100, knn_accuracy * 100, accuracy * 100],
    "Precision (%)": [precision_svm * 100, rf_precision * 100, knn_precision * 100, precision * 100],
    "Recall (%)": [recall_svm * 100, rf_recall * 100, knn_recall * 100, recall * 100],
    "F1-score (%)": [f1_svm * 100, rf_f1_score * 100, knn_f1_score * 100, f1 * 100]
}

# Define the metrics for quantum models
quantum_metrics = {
    "Model": ["VQC", "QCNN", "QSVM"],
    "Accuracy (%)": [accuracy * 100, accuracy_qcnn * 100, accuracy_qsvm * 100],
    "Precision (%)": [precision * 100, precision_qcnn * 100, precision_qsvm * 100],
    "Recall (%)": [recall * 100, recall_qcnn * 100, recall_qsvm * 100],
    "F1-score (%)": [f1 * 100, f1_qcnn * 100, f1_qsvm * 100]
}

# Create DataFrames for classical and quantum metrics
classical_df = pd.DataFrame(classical_metrics).set_index("Model").T
quantum_df = pd.DataFrame(quantum_metrics).set_index("Model").T

# Concatenate the DataFrames with a MultiIndex
combined_df = pd.concat([classical_df, quantum_df], axis=1, keys=['Classical', 'Quantum'])

# Print the combined DataFrame
print("Combined Performance Metrics for Classical and Quantum Models:")
print(combined_df)



"""# gpu"""

import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))



